import nltk
from nltk.corpus import gutenberg
import matplotlib.pyplot as plt
from collections import Counter
import string

# Завантаження тексту "Paradise Lost" з проекту Gutenberg
nltk.download('gutenberg')
milton_text = gutenberg.raw('milton-paradise.txt')

# Завантаження ресурсу для токенізації речень
nltk.download('punkt')

# Розбивка тексту на слова
words = nltk.word_tokenize(milton_text)

# Визначення кількості слів у тексті
word_count = len(words)
print("Кількість слів у тексті:", word_count)

# Підрахунок кількості вживання кожного слова
word_freq = Counter(words)

# Вибір 10 найбільш вживаних слів
top_10_words = word_freq.most_common(10)

# Розділення на слова та відповідні частоти
top_words, frequencies = zip(*top_10_words)

# Побудова стовпчастої діаграми
plt.figure(figsize=(10, 6))
plt.bar(top_words, frequencies, color='skyblue')
plt.xlabel('Слова')
plt.ylabel('Частота')
plt.title('10 найбільш вживаних слів у тексті')
plt.xticks(rotation=45)
plt.show()

# Завантаження стоп-слів та пунктуації
nltk.download('stopwords')
stop_words = set(nltk.corpus.stopwords.words('english'))
punctuation = set(string.punctuation)

# Видалення стоп-слів та пунктуації
filtered_words = [word.lower() for word in words if word.lower() not in stop_words and word.lower() not in punctuation]

# Підрахунок кількості вживання кожного слова після видалення стоп-слів та пунктуації
filtered_word_freq = Counter(filtered_words)

# Вибір 10 найбільш вживаних слів після видалення стоп-слів та пунктуації
filtered_top_10_words = filtered_word_freq.most_common(10)

# Розділення на слова та відповідні частоти
filtered_top_words, filtered_frequencies = zip(*filtered_top_10_words)

# Побудова стовпчастої діаграми
plt.figure(figsize=(10, 6))
plt.bar(filtered_top_words, filtered_frequencies, color='salmon')
plt.xlabel('Слова')
plt.ylabel('Частота')
plt.title('10 найбільш вживаних слів після видалення стоп-слів та пунктуації')
plt.xticks(rotation=45)
plt.show()
